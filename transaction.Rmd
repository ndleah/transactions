---
title: "Predictive Modelling on Financial Transactions"
author: "Leah Nguyen"
date: "`r Sys.Date()`"
output:
  html_document:
    fig_caption: yes
    smart: no
    number_sections: true
    toc: true
    toc_float: true
    code_folding: hide
---

# Introduction

```{r setup, echo=FALSE, 	message = FALSE,warning = FALSE}
knitr::opts_chunk$set(center = TRUE)
```

This article aims to analyse and provide insights from the monthly transaction data set in order to better understand the customer transaction patterns. The article also offers a study on linear regression model, an important concept in the field of machine learning, and discusses how this model can assist in the decision-making process of identifying trends in bank transactions within the years of 2013 - 2016.

To well capture this information, the CRISP-DM management model is adopted to provide a structured planning approach to a data mining project with 6 high-level phases. In particular, these phases assist companies in comprehending the data mining process and serve as a road map for planning and executing a data mining project (Medeiros, 2021). This study explores each of the six phases, and the tasks associated with each in the following orders:

* Business understanding
* Data understanding
* Data preparation
* Modeling
* Evaluation
* Deployment

```{r residuals, echo=FALSE, out.width="40%",fig.cap="Cross-Industry Standard Process for Data Mining (CRISP-DM project, 2000)",fig.align="center"}
knitr::include_graphics("img/CRISP-DM.png")
```

# Business understanding

**Business Understanding** is the first taken step in the CRISP-DM methodology. In this stage, the main task is to understand the purpose of the analysis and to provide a clear and crisp definition of the problem in respect of understanding the *Business objectives* and *Data mining objectives*.


In our case study, the posed question related Business object paraphrased from the sale manager's request is: “what is driving the trends and increase total monthly revenue?”. On the other hand, we wish to achieve the data mining object by applying data visualization tools to identify any underlying patterns from the dataset.

# Data Understanding

Following that, the **Data Understanding** or Exploratory Data Analysis (EDA) phase is where we focus on understanding the data collected to support the Business Understanding and resolve the business challenge (Wijaya, 2021). Visualization techniques play an essential role in this. Thus, The data was imported into the software package R to construct visualizations represented the findings found during the analysis.

Additionally, a two-stage approach is adopted to specify the content in this section, with [Stage 1](#edastage1) devoted to basic exploration and [Stage 2](#edastage2) devoted to univariate, bivariate, and multivariate analysis.


## Stage 1: Basic Exploration {#edastage1}

First, I will run the libraries which will be necessary for reading & manipulating our data and then conducting the graphs.

```{r, echo=FALSE, message = FALSE, warning = FALSE}
library(here)         # assess the file path
library(DataExplorer) # EDA visualizations
library(tidyverse)    # data wrangling
library(kableExtra)   # write table
library(ggplot2)      # data visualization
library(ggradar)      # plot seasonal trend
library(sqldf)        # using SQL
library(car)          # calculate the VIF 
library(dplyr)        # data processing
library(gganimate)    # create animated plots
library(ggpubr)       # comebine plots into single page
theme_set(theme_pubr())
library(reshape2)     # transpose table
library(fmsb)         # create radar chart
```

Once libraries are loaded, we explore the data with the goal of understanding its dimensions, data types, and distribution of values. In this assignment, a time series data set of financial transactions was used as the major source of data. The attributes information is specifically presented in Appendix []. As apparent from the table below, the data records 470,000+ observations across 5 columns, which are equivalent to 94,000+ bank transactions. The 5 features contained in this data set including `date`,	`customer_id`, `industry`,	`location`,	`monthly_amount`, clearly indicates the total transactions amounts for customers each month spanning a 3-year period over a range of industries and locations. Therefore, no further justification needs to be made on column names. 

```{r message=FALSE, center=TRUE}
# read the dataset and covert it into a df object
df <- read_csv(here("dataset/transactions.csv"))
df_overview <- introduce(df) %>%                  # Quick summary: index and column data types, non-null values and memory usage
  t()                                             # transpose the info for better display

df_overview %>%                                   
  kbl() %>%                                       # turn output into a table format
  kable_styling(bootstrap_options = "striped",    # apply bootstrap theme to the table
                full_width = F) 
```
It is also worthwhile to note that features are made up in multiple formats that include both numerical and time-series data. However, the output shows that the `date` column has the wrong data type which will need to be converted to date format later. 

```{r}
sapply(df, class) # inspect columns data type
```

Additionally, I investigate further by looking at the response field. Recall from the business question, we would expect to use the `monthly_amount` column as the target field since our goal is to get the predicted value of the monthly transaction value next month. Since the observation in this column are continuous, thus, I can conclude that our problem is defined as the supervised regression problem. Having known this information is extremely essential to select the right Machine Learning model in the later stage of this report. 

```{r fig.align="center", fig.cap="Missing values plot"}
# Plot the Quick summary information
plot_intro(df)
```
The illustration shows that there is no missing values on any fields of data. Nevertheless, some data sets define missing observations in categorical/character columns as a new category such as `"NA"`, `"NULL"`, etc. Thus, there are chances that we possibly miss these observations, which can lay a tremendous negative impact on the real data distribution. Consequently, a further address on the missing values of our categorical columns need to be made in order to confirm this observation.

The code output below interprets that there is no new missing value category exists in categorical columns. Thus, we can confirm our hypothesis that there is no missing values from both numerical and categorical columns in this data set.Furthermore, it also indicate that there are 1 row that contain odd value in `monthly_amount` column that will need to be resolved.

```{r}
# convert character values of character columns to upper case for better checking
missing_df <- data.frame(lapply(df, function(v) {
  if (is.character(v)) return(toupper(v))
  else return(v)
}))

# check if there is there is missing values assigned under new category
## date column
sprintf(paste0("Is there any missing value observation categories in date column (T/F)?: ", 
               missing_df[1] %in% c("NA","N/A","NULL","")))
## customer_id column
sprintf(paste0("Is there any missing value observation categories in customer_id column (T/F)?: ", 
               missing_df[1] %in% c("NA","N/A","NULL","")))

# Check for any transaction with zero values
sprintf(paste0("How many rows contained 0 value in monthly transaction amount?: ", 
               sum(df$monthly_amount==0)))
```
##  Stage 2: Univariate, Bivariate & Multivariate Analysis {#edastage2}

### Univariate: Check the distribution of each field

In research from Sharma (2020), the distributions of the independent variable and the target variable are assumed to be similar in linear models. Therefore, understanding the skewness of data helps us in creating better linear models. 

However, before doing that, the previous issues indicated in this dataset need to be addressed:

1. There is a wrong data types in `Date` column. Convert it into the right format\
2. \
3. The number in transaction

```{r}
############################
# DATA TRANSFORMATION
############################
# convert date column into the date format
df$date <- as.Date(df$date,"%d/%m/%Y")

# convert customer_id column into character format
df$customer_id = as.character(df$customer_id, format = "")

# convert location column into character format
df$location <- as.character(df$location)

# convert industry column into character format
df$industry <- as.character(df$industry)
```

Next, let’s visualise the dataset to see which group of industry and location statistically contribute the most to the significant difference.

```{r fig.align="center", fig.cap="Data distribution", message=FALSE, warning=FALSE}
## plot data distribution of MONTHLY_AMOUNT group by INDUSTRY
par(mfrow=c(1,2)) # combine 2 plots into 1 plot

hist(df$industry, # create historgram
     main = "Trans by Industry", 
     xlab="Industry", 
     xlim = c(0,10), 
     ylim=c(0,50000), 
     las=0)

## plot data distribution of MONTHLY_AMOUNT group by LOCATION
hist(df$location, # create histogram
     main = "Trans by Location", 
     xlab="Location", 
     xlim = c(0,10), 
     ylim=c(0,50000), 
     las=0)
```

As can be seen from the plot, the location 1 and 2 made the top contributions for the `Industry` column while the industry 2 and industry 1 occupied for the highest frequency distribution for the `Location` column. These results imply that the model can perform better at predicting the total transaction amount for next month with location 1, 2 and/or industry 1, 2.

Having known this information is essentially important to gain better understandings about the transaction data set and provide great insights for transforming data in the later stage.


# Data preparation

The goal will be to build a model that can predict future monthly_amounts.

We will need to add a new variable called “time_number”, which is an integer describing the date order of rows in the dataset. For example: January, 2013 becomes 1, February 2013 becomes 2 etc. We will then do a rough 70:30 test:train data split, ensuring that the lower values seen across December are represented in our testing set.

Aggregate the data, grouping by date, industry and location, and calculating the mean monthly_amount

```{r fig.align="center", message=FALSE, warning=FALSE}
#######################################################
# DATA PREPARATION
#######################################################

############################
# DATA TABLE TRANSFORMATION
############################
# create new df contain total transaction amount
transaction_amount <- sqldf(
"SELECT
  date,
  'Transaction Amount' as type,             -- specify value type
  SUM(monthly_amount) AS value              -- sum total transaction amount
FROM df
GROUP BY date                                -- filter by date
ORDER BY date
"
)

# create new df contain number of transaction
transaction_count <- sqldf(
"SELECT
  date,
    'Transaction Count' as type,             -- specify value type
  COUNT(*) as value                         -- count total number of transactions
FROM df
GROUP BY date                                -- filter by date
ORDER BY date
"
)

# merge 2 df into 1 new TRANSACTION df vertically 
transaction_df <- rbind(transaction_amount, 
                        transaction_count)

# create new INDUSTRY df contain total transaction by industry over time
industry <- sqldf(
"SELECT
  date,
  industry,
  SUM(monthly_amount) AS transaction_amount, -- sum total transaction amount
  COUNT(*) as transaction_count              -- count total number of transactions
FROM df
GROUP BY 
  date,                                      -- filter by date
  industry                                   -- filter by industry
ORDER BY date
"
)

# create new LOCATION df contain total transaction by location over time
location <- sqldf(
"SELECT
  date,
  location,
  SUM(monthly_amount) AS transaction_amount, -- sum total transaction amount
  COUNT(*) as transaction_count              -- count total number of transactions
FROM df
GROUP BY 
  date,                                      -- filter by date
  location                                   -- filter by location
ORDER BY date
"
)
```

```{r}
#Mutate the dataset to create a new column called 'monthly_amount_above_mean'  
# transactions_with_mean_boolean <- transactions %>%
#   # creating a new variable to classify transaction column
#   mutate(monthly_amount_above_mean = ifelse(monthly_amount > mean(monthly_amount), TRUE, FALSE))
```

```{r fig.align="center", fig.cap="Transaction amount vs. transaction number trend over time", message=FALSE, warning=FALSE}
#################################################################
# # Data Visualization: Transaction amount vs. transaction number
#################################################################
# plot transaction amount over time
monthly_amount_plot <- transaction_df %>% 
  filter(type=="Transaction Amount") %>%        # filter by transaction amount only
  ggplot(aes(x = date, y = value/1e6)) +        # assign x and y-axis from the dataset
  geom_line(color = "indianred", size=1.6) +    # add the line graph, color, and the size
  geom_smooth(formula = y~x,                    # the relationship graph between x and y
              method = 'loess') +
  labs(x = "Year", 
       y = "Total transaction amount (in millions)",
       title = "Monthly Transaction Amount",
       subtitle = "2013 to 2016") +
  theme_minimal()

# plot total transaction number over time
monthly_count_plot <- transaction_df %>% 
  filter(type=="Transaction Count") %>%         # filter by total transaction number count only
  ggplot(aes(x = date, y = value)) +            # assign x and y-axis from the dataset
  geom_line(color = "indianred", size=1.6) +    # add the line graph, color, and the size
  geom_smooth(formula = y~x,                    # the relationship graph between x and y
              method = 'loess') +
  labs(x = "Year", 
       y = "Total transaction number",
       title = "Total Transaction Number",
       subtitle = "2013 to 2016") +
  theme_minimal()

## combine individual plots into a single page  
ggarrange(monthly_amount_plot,
          monthly_count_plot,
          ncol = 2, nrow = 1)
```

Number of transactions and total amount of sales rose sharply throughout the years, from 2013 to 2017. The seasonal trend can be found on total amount of sales while the up trend for number of transactions is quite smooth. 


```{r fig.align="center", fig.cap="Seasonal Trend Over the Years", message=FALSE, warning=FALSE}
############################################################
# Data Visualization: Seasonal Trend Over the Years
############################################################
# Create new MONTHLY_TREND df to plot seasonal transaction trend
set.seed(99)
new_ts_df <- sqldf(
"SELECT
   strftime('%m', date) as month,   --extract month from date column                   
   strftime('%Y',                   --extract year from date column
   date * 3600 * 24,
   'unixepoch') as year,
  SUM(monthly_amount) AS transaction_amount
FROM df
GROUP BY
  month,
  year
ORDER BY
  month,
  year
"
)

# transpose the dataset to prepare for the data visulization
monthly_trend <- recast(new_ts_df, 
                        year + variable ~ month, 
                        id.var = c("month", "year"))
monthly_trend <- data.frame(monthly_trend[,-1],            # use the first column as the data index
                            row.names = monthly_trend[,1]) # use the first row as the header
monthly_trend <- subset(monthly_trend, select = -variable) # remove the unecessary column

# create new vector specify month column names
colnames(monthly_trend) <- c('January', 'February', 'March', 'April', 
                             'May', 'June', 'July', 'August', 'September', 
                             'October', 'November', 'December')

##################################
## RADAR CHART FOR ALL YEARS
################################## 

# To use the fmsb package, I have to add 2 lines to the dataframe: the max and min of each variable to show on the plot!
data <- rbind(rep(1400e6,12) , rep(0,12) , monthly_trend)
 
# Color vector
colr_1 <- rgb(0.2,0.5,0.5,0.9)
colr_2 <- rgb(0.8,0.2,0.5,0.9)
colr_3 <- rgb(0.7,0.5,0.1,0.9)
colr_4 <- "#FC4E07"

# set color theme for radar border
colors_border=c(colr_1, 
                colr_2, 
                colr_3, 
                colr_4)

# plot with default options:
seasonal_mul_plot <- 
  radarchart(data, axistype=0,
    #custom polygon
    pcol=colors_border, plwd=2 , plty=1,
    #custom the grid
    cglcol="grey", cglty=1, axislabcol="grey", cglwd=0.8,
    #custom labels
    vlcex=0.8, title=paste("Transaction Seasonal Trend"), cex.main = 1
    )
# Add a legend
legend(seasonal_mul_plot, x=1.5, y=0.5, legend = rownames(data[-c(1,2),]), 
       bty = "n", pch=15 , col=colors_border, text.col = "black", cex=1.2, pt.cex=1, title = "Year")

##################################
## BAR CHART FOR INDIVIDUAL YEARS
################################## 

ggplot(new_ts_df) +
 aes(x = month, fill = year, weight = transaction_amount/1e6) +
 geom_bar() +
 scale_fill_brewer(palette = "Dark2", 
 direction = -1) +
 labs(y = "Total transaction amount (in millions)", title = "Monthly Seasonal Trend", 
 subtitle = "Individual Year (from 2013 to 2016)") +
 coord_flip() +
 theme_minimal() +
 theme(legend.position = "none", 
 plot.title = element_text(size = 15L)) +
 facet_wrap(vars(year), scales = "free", nrow = 1L)
```



```{r fig.align="center", fig.cap="Transaction amount by Location vs. Industry", message=FALSE, warning=FALSE}
#######################################################
# Monthly transaction amount by country and location over time
#######################################################

# plot transaction info by industry
industry_amount_plot <- location_amount_plot <- ggplot(industry) +
  aes(x = date, y = transaction_amount, colour = industry) +
  geom_line(size = 1) +
  scale_color_hue(direction = 1) +
  labs(x = "Year", 
       y = "Total transaction amount (in millions)",
       title = "Transaction Amount",
       subtitle = "By industry (from 2013 to 2016)") +
  theme_minimal()


# plot transaction info by location
location_amount_plot <- ggplot(location) +
  aes(x = date, y = transaction_amount, colour = location) +
  geom_line(size = 1) +
  scale_color_hue(direction = 1) +
  labs(x = "Year", 
       y = "Total transaction amount (in millions)",
       title = "Transaction Amount",
       subtitle = "By location (from 2013 to 2016)") +
  theme_minimal()

# combine plots into a single page  
ggarrange(industry_amount_plot, location_amount_plot,
          ncol = 2, nrow = 1) 

```

It is not surprising that total sales of location 1 and 2 increased significantly compared to other locations. Meanwhile, in terms of industry, industry 2, 3 and 1 shows a rapid growing over the years while others’ progress are quite slow.



<!-- Here the dark squares represent a strong correlation (close to 1) while the lighter ones represent the weaker correlation(close to 0). That’s the reason, all the diagonals are dark blue, as a variable is fully correlated with itself. -->
<!-- Now, the thing worth noticing here is that the correlation between newspaper and radio is 0.35. This indicates a fair relationship between newspaper and radio budgets. Hence, it can be inferred that → when the radio budget is increased for a product, there’s a tendency to spend more on newspapers as well. -->
<!-- This is called collinearity and is referred to as a situation in which two or more input variables are linearly related. -->
<!-- Hence, even though the Multiple Regression model shows no impact on sales by the newspaper, the Simple Regression model still does due to this multicollinearity and the absence of other input variables. -->


# Modeling
```{r regression models, eval=FALSE, include=FALSE}
#fit the regression model
model <- lm(monthly_amount ~ ., data = df)

#view the output of the regression model
summary(model)
```

```{r eval=FALSE, include=FALSE}
# calculate the VIF for each predictor variable in the model

vif(model)

```

# Evaluation

<!-- Checking Linear Regression Assumptions -->

Linear regression makes several assumptions about the data, such as:

Linearity of the data
Normality of residuals
Homogeneity of residuals variance - whether variation of observations around the regression line is constant
Independence of residuals error terms
All these assumptions can be checked by producing some diagnostic plots visualizing the residual errors. We would like to analyze model3 as it is the best model according to ANOVA.

```{r}
# par(mfrow = c(2, 2))
# plot(model3)
```

* **For linearity assumption:** look at Residuals vs.Fitted plot. A horizontal line, without distinct patterns is an indication for a linear relationship is a good sign in this graph. For model3 : It is can be clearly seen on our plot!
* **For normality assumption:** look at Q-Q plot. If residuals are normally distributed, the residuals points are supposed to follow the straight dashed line. For model3 : Yay! Looks like this plot shows good stuff!
* **For homoscedasticity assumption:** look at Scale-Location & Residuals vs. Leverage plots. In this case horizontal line with equally spread points is a good indication of homoscedasticity. For model3: Once again, no problems with our graph!
* **To identify outliers, which extreme values might influence the regression results can be identified with the Residuals vs Leverage plot. Here we can spot only one of them.

# Deployment


# References

1. Medeiros, L. (2021, December 19). The CRISP-DM methodology - Lucas Medeiros. Medium. https://medium.com/@lucas.medeiross/the-crisp-dm-methodology-d1b1fc2dc653

2. Sharma, A. (2020, December 23). What is Skewness in Statistics? | Statistics for Data Science. Analytics Vidhya. https://www.analyticsvidhya.com/blog/2020/07/what-is-skewness-statistics/Wijaya, C. Y. (2021, December 19). 

3. CRISP-DM Methodology For Your First Data Science Project. Medium. https://towardsdatascience.com/crisp-dm-methodology-for-your-first-data-science-project-769f35e0346c

# Appendix


## Appendix 1: Data Description


```{r message=FALSE, warning=FALSE, center=TRUE}
## load data description csv file
dd <- read_csv(here("dataset/data_description.csv"))

# display the information under table format
dd %>%
  kbl() %>%
  kable_styling(full_width = F)
```
